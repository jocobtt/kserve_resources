{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp \n",
    "import kserve \n",
    "from kfp import dsl \n",
    "from kfp.components import func_to_container_op, OutputPath, InputPath\n",
    "from kfp import components\n",
    "import os \n",
    "from functools import partial\n",
    "# https://www.kubeflow.org/docs/distributions/azure/azureendtoend/\n",
    "# https://medium.com/kubeflow/an-end-to-end-ml-pipeline-on-prem-notebooks-kubeflow-pipelines-on-the-new-minikf-33b7d8e9a836\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    kfp.create_component_from_func,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"]\n",
    ")\n",
    "def process(train_x_path: OutputPath(\"pickle\"),\n",
    " train_y_path: OutputPath(\"pickle\"), \n",
    " test_x_path: OutputPath(\"pickle\"), \n",
    " test_y_path: OutputPath(\"pickle\")):\n",
    "    import pandas as pd \n",
    "    from sklearn.datasets import load_boston\n",
    "    import os, pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    boston_df = load_boston()\n",
    "    boston = pd.DataFrame(boston_df.data, columns=boston_df.feature_names)\n",
    "    \n",
    "    X=boston.iloc[:,0:-1]\n",
    "    y=boston.iloc[:,-1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "    # output the data to the load_data_path\n",
    "    with open(train_x_path, \"wb\") as f:\n",
    "        pickle.dump(X_train, f)\n",
    "    with open(train_y_path, \"wb\") as f:\n",
    "        pickle.dump(y_train, f)\n",
    "    with open(test_x_path, \"wb\") as f:\n",
    "        pickle.dump(X_test, f)\n",
    "    with open(test_y_path, \"wb\") as f:\n",
    "        pickle.dump(y_test, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train step \n",
    "@partial(\n",
    "    kfp.create_component_from_func,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install = [\"pandas\", \"scikit-learn\", \"mlflow\", \"joblib\"],\n",
    ")\n",
    "\n",
    "def train(train_x_path: InputPath(\"pickle\"),\n",
    " train_y_path: InputPath(\"pickle\"), \n",
    " test_x_path: InputPath(\"pickle\"),\n",
    " test_y_path: InputPath(\"pickle\"),\n",
    " model_path: OutputPath(\"dump\")): # input stuff about our model env here too? \n",
    "    import pandas as pd \n",
    "    import os, pickle\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from joblib import dump\n",
    "    import sklearn.metrics as metrics\n",
    "    #import mlflow \n",
    "\n",
    "    with open(train_x_path, mode = \"rb\") as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(train_y_path, mode = \"rb\") as f:\n",
    "        y_train = pickle.load(f)\n",
    "    with open(test_x_path, mode = \"rb\") as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(test_y_path, mode = \"rb\") as f:\n",
    "        y_test = pickle.load(f)\n",
    "\n",
    "    # train our model \n",
    "    regressor = DecisionTreeRegressor(random_state=42, max_depth=5)\n",
    "    \n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    with open(model_path, mode = \"wb\") as f:\n",
    "        dump(regressor, f)\n",
    "    #dump(regressor, 'boston_model.joblib')\n",
    "\n",
    "    # evaluate our model\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    #metrics.r2_score(y_test, y_pred)\n",
    "    sk_mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    # output here to compare so that we can then decide which to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenger model \n",
    "@partial(\n",
    "    kfp.create_component_from_func,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install = [\"pandas\", \"lightgbm\", \"scikit-learn\", \"mlflow\", \"joblib\"],\n",
    ")\n",
    "\n",
    "def train(train_x_path: InputPath(\"pickle\"),\n",
    "train_y_path: InputPath(\"pickle\"),\n",
    "test_x_path: InputPath(\"pickle\"),\n",
    "test_y_path: InputPath(\"pickle\"), \n",
    "model_path: OutputPath(\"dump\")):\n",
    "    import pandas as pd \n",
    "    import os, pickle\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    import lightgbm as lgb\n",
    "    from joblib import dump\n",
    "    import sklearn.metrics as metrics\n",
    "    #import mlflow \n",
    "\n",
    "    with open(train_x_path, mode = \"rb\") as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(train_y_path, mode = \"rb\") as f:\n",
    "        y_train = pickle.load(f)\n",
    "    with open(test_x_path, mode = \"rb\") as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(test_y_path, mode = \"rb\") as f:\n",
    "        y_test = pickle.load(f)\n",
    "\n",
    "    train_set = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=train_set)\n",
    "    # define our parameters \n",
    "    params = {\n",
    "    'task': 'train', \n",
    "    'boosting': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'num_leaves': 10,\n",
    "    'learnnig_rage': 0.05,\n",
    "    'metric': {'l2','l1'},\n",
    "    'verbose': -1\n",
    "    }\n",
    "    model = lgb.train(params, train_set, \n",
    "    num_boost_round=100, \n",
    "    valid_sets = lgb_eval,\n",
    "    early_stopping_rounds=10)\n",
    "\n",
    "# https://www.datatechnotes.com/2022/03/lightgbm-regression-example-in-python.html\n",
    "\n",
    "    with open(model_path, mode = \"wb\") as f:\n",
    "        dump(model, f)\n",
    "\n",
    "    # evaluate our model\n",
    "    y_pred = model.predict(X_test)\n",
    "    #metrics.r2_score(y_test, y_pred)\n",
    "    lgb_mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    # output here to compare so that we can then decide which to use\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation and interpretation step \n",
    "@partial(\n",
    "    kfp.create_component_from_func,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install = [\"pandas\", \"scikit-learn\", \"mlflow\", \"joblib\", \"shap\", \"matplotlib\"],\n",
    ")\n",
    "\n",
    "def evaluate(model_path: InputPath(\"dump\"),\n",
    "    test_x_path: InputPath(\"pickle\"),\n",
    "    test_y_path: InputPath(\"pickle\"),\n",
    "    lime_path: OutputPath(\"pickle\")):\n",
    "        import pandas as pd \n",
    "        import os, pickle\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        from joblib import dump, load\n",
    "        import sklearn.metrics as metrics\n",
    "        import matplotlib.pyplot as plt\n",
    "        import lime\n",
    "        import lime.lime_tabular\n",
    "    \n",
    "        with open(model_path, mode=\"rb\") as file_reader:\n",
    "            model = load(file_reader)\n",
    "        with open(test_x_path, mode = \"rb\") as f:\n",
    "            X_test = pickle.load(f)\n",
    "        with open(test_y_path, mode = \"rb\") as f:\n",
    "            y_test = pickle.load(f)\n",
    "\n",
    "        # evaluate the model \n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=boston.feature_names, class_names=['price'], categorical_features=categorical_features, verbose=True, mode='regression')\n",
    "\n",
    "        i = 25\n",
    "        exp = explainer.explain_instance(X_test[i], model.predict, num_features=5)\n",
    "\n",
    "        exp.as_list()\n",
    "\n",
    "        with open(lime_path, mode=\"wb\") as file_writer:\n",
    "            #pickle.dump(exp, file_writer)\n",
    "            exp.save_to_file(file_writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload step - also upload model artifact to our location - s3 bucket or something \n",
    "@partial(\n",
    "    kfp.create_component_from_func,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install = [\"kserve\", \"kubernetes\", \"mlflow\", \"mlflow\", \"joblib\", \"boto3\"],\n",
    ")\n",
    "\n",
    "def upload_mod(model_name: str,\n",
    "    model_path: InputPath(\"dump\"),\n",
    "    ):\n",
    "    import os, pickle\n",
    "    import kserve\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    import mlflow \n",
    "    from mlflow.tracking.client import MlflowClient\n",
    "    import boto3\n",
    "    import joblib\n",
    "\n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.mlflow-system.svc:9000\"\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = secret[\"AWS_ACCESS_KEY_ID\"]\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "    client = MlflowClient(\"http://mlflow-service.mlflow-system.svc:5000\")\n",
    "    mlflow.set_tracking_uri(\"http://mlflow-service.mlflow-system.svc:5000\")\n",
    "    with open(model_path, mode=\"rb\") as file_reader:\n",
    "        model = joblib.load(file_reader)\n",
    "    mlflow.pyfunc.save_model(python_model = model, \n",
    "        path = model_name, conda_env = conda_env,) # check this part \n",
    "    \n",
    "    run = client.create_run(experiment_id=\"0\") # create run\n",
    "    client.log_artifact(run.info.run_id, model_name)\n",
    "    with mlflow.start_run(run_name=run.info.run_id) as run_: # regist model\n",
    "        mlflow.pyfunc.log_model(\n",
    "            python_model=model,\n",
    "            artifact_path=model_name,\n",
    "            registered_model_name=\"BCI-Model\",\n",
    "        )\n",
    "\n",
    "    # how to upload mlflow model to s3 bucket?\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kserve portion \n",
    "@ partial(\n",
    "    kfp.create_component_from_func,\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install = [\"kserve\", \"kubernetes\", \"mlflow\", \"mlflow\", \"joblib\", \"boto3\"],\n",
    ")\n",
    "def serve_model(model_name: str,\n",
    "model_path: InputPath(\"s3\"), \n",
    "namespace: str = \"kserve-test\",\n",
    "action: str = \"apply\",\n",
    "framework: str = \"sklearn\",\n",
    "):\n",
    "    import os, pickle\n",
    "    import kserve\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kfp import components\n",
    "\n",
    "    \n",
    "    kserve_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n",
    "                                               'master/components/kserve/component.yaml')\n",
    "\n",
    "\n",
    "    kserve_op(action = action, model_name = model_name, model_uri = model_path, namespace = namespace, framework = framework).set_image_pull_policy('Always')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end to end pipeline definition - may need to adjust some of this \n",
    "@dsl.pipeline(\n",
    "    name = \"end to end pipeline\",\n",
    "    description = \"end to end pipeline that ...\"\n",
    ")\n",
    "# https://github.com/jerife/MLOps-on-kubernetes/blob/main/pipeline.py\n",
    "def end_to_end_pipeline():\n",
    "    data_process = dsl.ContainerOp(\n",
    "        name = \"data_processing\",\n",
    "        image = \"jacobkun/red_etl:alpha\",\n",
    "        command = [\"python\", \"data_process.py\"],\n",
    "\n",
    "    )\n",
    "\n",
    "    model_train = dsl.ContainerOp(\n",
    "        name = \"model_training\",\n",
    "        image = \"jacobkun/model_train:alpha\",\n",
    "        command = [\"python\", \"model_train.py\"],\n",
    "    )\n",
    "\n",
    "    model_upload = dsl.ContainerOp(\n",
    "        name = \"model_upload\",\n",
    "        image = \"jacobkun/model_upload:alpha\",\n",
    "        command = [\"python\", \"model_upload.py\"],\n",
    "    )\n",
    "\n",
    "    serve_model = dsl.ContainerOp(\n",
    "        name = \"model_deploy\",\n",
    "        image = \"jacobkun/model_deploy:alpha\",\n",
    "        command = [\"python\", \"model_deploy.py\"],\n",
    "    )\n",
    "\n",
    "    # this will be for performance testing etc. - still need to think through this part a bit more..\n",
    "    model_check = dsl.ContainerOp(\n",
    "        name = \"model_check\",\n",
    "        image = \"jacobkun/model_check:alpha\",\n",
    "        command = [\"python\", \"model_check.py\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline \n",
    "if __name__ == \"__main__\":\n",
    "    kfp.compiler.Compiler().compile(end_to_end_pipeline, \"end_to_end.yaml\")\n",
    "\n",
    "    # Submit the pipeline to the client\n",
    "    with open(os.environ['KF_PIPELINES_SA_TOKEN_PATH'], \"r\") as f:\n",
    "        TOKEN = f.read()\n",
    "\n",
    "    client = kfp.Client(\n",
    "        host='http://ml-pipeline.kubeflow.svc.cluster.local:8888',\n",
    "        existing_token=TOKEN)\n",
    "    # create our experiment \n",
    "    client.create_experiment(\"end_to_end\")\n",
    "    client.create_run_from_pipeline_func(end_to_end_pipeline, arguments = {}, experiment_name = \"end_to_end\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}